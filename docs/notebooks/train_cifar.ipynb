{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "\n",
    "import deel.lipdp.layers as DP_layers\n",
    "import deel.lipdp.losses as DP_losses\n",
    "from deel.lipdp.pipeline import bound_clip_value\n",
    "from deel.lipdp.pipeline import load_and_prepare_data\n",
    "from deel.lipdp.sensitivity import get_max_epochs\n",
    "from deel.lipdp.model import DP_Accountant\n",
    "from deel.lipdp.model import DP_Sequential\n",
    "from deel.lipdp.model import DPParameters\n",
    "from deel.lipdp.model import AdaptiveLossGradientClipping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data :\n",
    "\n",
    "It is important to import the data with the right DP parameters to account properly for the privacy guarantees of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = False \n",
    "\n",
    "if augment : \n",
    "    augmentations = [\n",
    "        keras_cv.layers.RandomRotation(0.2, fill_mode=\"reflect\", interpolation=\"bilinear\"),\n",
    "        keras_cv.layers.RandomTranslation(\n",
    "            0.2, 0.2, fill_mode=\"reflect\", interpolation=\"bilinear\"\n",
    "        ),\n",
    "    ]\n",
    "else : \n",
    "    augmentations = None \n",
    "    \n",
    "ds_train, ds_test, dataset_metadata = load_and_prepare_data(\n",
    "    \"cifar10\",\n",
    "    batch_size=750,\n",
    "    colorspace=\"HSV\",\n",
    "    augmentations=augmentations,\n",
    "    drop_remainder=True,  # accounting assumes fixed batch size\n",
    "    bound_fct=bound_clip_value(\n",
    "        10.0\n",
    "    ),  # clipping preprocessing allows to control input bound\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pay attention to the fact that the effective batch size in memory will be batch_size $\\times$ len(augmentations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the DP parameters :\n",
    "\n",
    "We also need to declare explicitly the parameters of the DP training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_parameters = DPParameters(\n",
    "    noisify_strategy=\"global\",\n",
    "    noise_multiplier=0.65,\n",
    "    delta=1e-5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model :\n",
    "\n",
    "We use a simple convolutive network to classify on the MNIST dataset. We add a loss gradient clipping layer at the end of our network for more tightness on our gradient's upper bound. Therefore allowing for better results with one less hyperparameter to tune for dynamically chosen clipping constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas.massena/Code/DEBUG/dp-lipschitz/lipdp_dev_env/lib/python3.9/site-packages/deel/lip/model.py:74: UserWarning: Sequential model contains a layer which is not a 1-Lipschitz layer: dp__bounded_input_7\n",
      "  warn(_msg_not_lip.format(layer.name))\n",
      "/home/thomas.massena/Code/DEBUG/dp-lipschitz/lipdp_dev_env/lib/python3.9/site-packages/deel/lip/model.py:74: UserWarning: Sequential model contains a layer which is not a 1-Lipschitz layer: dp__clip_gradient_7\n",
      "  warn(_msg_not_lip.format(layer.name))\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    DP_layers.DP_BoundedInput(\n",
    "        input_shape=dataset_metadata.input_shape,\n",
    "        upper_bound=dataset_metadata.max_norm,\n",
    "    ),\n",
    "    DP_layers.DP_SpectralConv2D(\n",
    "        filters=32, kernel_size=3, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_Flatten(),\n",
    "    DP_layers.DP_SpectralDense(\n",
    "        units=512, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_GroupSort(2),\n",
    "    DP_layers.DP_SpectralDense(\n",
    "        units=10, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_ClipGradient(\n",
    "        epsilon=1, mode=\"dynamic_svt\", patience=5\n",
    "    )\n",
    "]\n",
    "\n",
    "model = DP_Sequential(\n",
    "    layers=layers, dp_parameters=dp_parameters, dataset_metadata=dataset_metadata\n",
    ")\n",
    "\n",
    "loss = DP_losses.DP_TauCategoricalCrossentropy(tau=14.5)\n",
    "\n",
    "# Compatible with any kind of non-private optimizer : \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=opt,\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the desired DP guarantees :\n",
    "\n",
    "We compute the budget of epochs needed to yields the DP guarantees that you desire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch bounds = (0, 512.0) and epsilon = 157.345818063821 at epoch 512.0\n",
      "epoch bounds = (0, 256.0) and epsilon = 84.46997949773441 at epoch 256.0\n",
      "epoch bounds = (0, 128.0) and epsilon = 47.00231900480946 at epoch 128.0\n",
      "epoch bounds = (0, 64.0) and epsilon = 28.268488758346994 at epoch 64.0\n",
      "epoch bounds = (0, 32.0) and epsilon = 19.269236467143514 at epoch 32.0\n",
      "epoch bounds = (0, 16.0) and epsilon = 13.209832765975165 at epoch 16.0\n",
      "epoch bounds = (0, 8.0) and epsilon = 8.754985809645154 at epoch 8.0\n",
      "epoch bounds = (4.0, 8.0) and epsilon = 6.5275623314801505 at epoch 4.0\n",
      "epoch bounds = (4.0, 6.0) and epsilon = 8.06463748526166 at epoch 6.0\n",
      "epoch bounds = (5.0, 6.0) and epsilon = 7.7194633230699115 at epoch 5.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = get_max_epochs(8.0, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model : \n",
    "\n",
    "The training process is called through the model.fit attribute. We use the following callbacks : \n",
    "\n",
    "- **DP_Accountant** (log_fn) : accounts for the privacy guarantees after each epoch of training (*log_fn* makes it compatible with W&B logging).\n",
    "- **DP_AdaptiveGradientClipping** (ds_train, patience) : automatically updates the losses's gradient clipping constant every *patience* steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On train begin : \n",
      "Initial value is now equal to lipschitz constant of loss:  tf.Tensor(1.4142135, shape=(), dtype=float32)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.2395\n",
      " (5.146865646830758, 1e-05)-DP guarantees for epoch 1 \n",
      "\n",
      "updated_clip_value :  1.006512355202473\n",
      "66/66 [==============================] - 15s 196ms/step - loss: 0.1454 - accuracy: 0.2395 - val_loss: 0.1378 - val_accuracy: 0.3011\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.3112\n",
      " (5.492039853807592, 1e-05)-DP guarantees for epoch 2 \n",
      "\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.1351 - accuracy: 0.3112 - val_loss: 0.1326 - val_accuracy: 0.3309\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.3325\n",
      " (5.83721401562529, 1e-05)-DP guarantees for epoch 3 \n",
      "\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.1314 - accuracy: 0.3325 - val_loss: 0.1302 - val_accuracy: 0.3443\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.3429\n",
      " (6.182388177442989, 1e-05)-DP guarantees for epoch 4 \n",
      "\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.1294 - accuracy: 0.3429 - val_loss: 0.1283 - val_accuracy: 0.3574\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.3521\n",
      " (7.374289160878165, 1e-05)-DP guarantees for epoch 5 \n",
      "\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.1280 - accuracy: 0.3521 - val_loss: 0.1273 - val_accuracy: 0.3622\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    DP_Accountant(log_fn=\"logging\"),\n",
    "    AdaptiveLossGradientClipping(\n",
    "        ds_train=ds_train\n",
    "    ),  # DO NOT USE THIS CALLBACK WHEN mode != \"dynamic_svt\"\n",
    "]\n",
    "\n",
    "hist = model.fit(\n",
    "    ds_train,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipdp_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
