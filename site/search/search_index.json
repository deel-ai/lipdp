{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Conventionally, Differentially Private ML training relies on Gradient Clipping to guarantee verifiable privacy guarantees. By using 1-Lipschitz networks developped by the deel-lip project. We can propose a new alternative to gradient clipping based DP ML. Indeed, by theoretically bounding the value of the sensitivity of our 1-Lipschitz layers, we can directly calibrate a batchwise noising of the gradients to guarantee (epsilon,delta)-DP.</p>"},{"location":"#table-of-contents","title":"\ud83d\udcda Table of contents","text":"<ul> <li>\ud83d\udcda Table of contents</li> <li>\ud83d\udd25 Tutorials</li> <li>\ud83d\ude80 Quick Start</li> <li>\ud83d\udce6 What's Included</li> <li>\ud83d\udc4d Contributing</li> <li>\ud83d\udc40 See Also</li> <li>\ud83d\ude4f Acknowledgments</li> <li>\ud83d\udc68\u200d\ud83c\udf93 Creator</li> <li>\ud83d\uddde\ufe0f Citation</li> <li>\ud83d\udcdd License</li> </ul>"},{"location":"#tutorials","title":"\ud83d\udd25 Tutorials","text":""},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Libname requires some stuff and several libraries including Numpy. Installation can be done using Pypi:</p> <pre><code>pip install dist/lipdp-0.0.1a0-py2.py3-none-any.whl[dev]\n</code></pre> <p>Now that lipdp is installed, here are some basic examples of what you can do with the  available modules.</p>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>Code can be found in the <code>lipdp</code> folder, the documentation ca be found by running  <code>mkdocs build</code> and <code>mkdocs serve</code> (or loading <code>site/index.html</code>). Experiments were   done using the code in the <code>experiments</code> folder.</p>"},{"location":"#contributing","title":"\ud83d\udc4d Contributing","text":"<p>Feel free to propose your ideas or come and contribute with us on the Libname toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here.</p>"},{"location":"#pre-commit-conventional-commits-100","title":"pre-commit : Conventional Commits 1.0.0","text":"<p>The commit message should be structured as follows:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>The commit contains the following structural elements, to communicate intent to the consumers of your library:</p> <ul> <li> <p>fix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in Semantic Versioning).</p> </li> <li> <p>feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in Semantic Versioning).</p> </li> <li> <p>BREAKING CHANGE: a commit that has a footer BREAKING CHANGE:, or appends a ! after the type/scope, introduces a breaking API change (correlating with MAJOR in Semantic Versioning). A BREAKING CHANGE can be part of commits of any type.</p> </li> <li> <p>types other than fix: and feat: are allowed, for example @commitlint/config-conventional (based on the the Angular convention) recommends build:, chore:, ci:, docs:, style:, refactor:, perf:, test:, and others.</p> </li> <li> <p>footers other than BREAKING CHANGE:  may be provided and follow a convention similar to git trailer format. <li> <p>Additional types are not mandated by the Conventional Commits specification, and have no implicit effect in Semantic Versioning (unless they include a BREAKING CHANGE). A scope may be provided to a commit\u2019s type, to provide additional contextual information and is contained within parenthesis, e.g., feat(parser): add ability to parse arrays.</p> </li>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":""},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>If you want to highlights the main contributors</p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":""},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under MIT license.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"CONTRIBUTING/#setup-with-make","title":"Setup with make","text":"<ul> <li>Clone the repo <code>git clone https://github.com/anonymized-ai/dp-lipschitz.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd lipdp</code></li> <li>Create a virtual environment and install the necessary dependencies for development:</li> </ul> <p><code>make prepare-dev &amp;&amp; source lipdp_dev_env/bin/activate</code>.</p> <p>Welcome to the team !</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>To run test <code>make test</code> This command activate your virtual environment and launch the <code>tox</code> command.</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8</p> <p>Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the anonymized main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy).</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style and run <code>make check_all</code> to check all files format.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"api/layers/","title":"lipdp.layers module","text":""},{"location":"api/layers/#lipdp.layers.AddBias","title":"<code>AddBias</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Adds a bias to the input.</p> <p>Remark: the euclidean norm of the bias must be bounded in advance. Note that this is the euclidean norm of the whole bias vector, not the norm of each element of the bias vector.</p> <p>Warning: beware zero gradients outside the ball of norm norm_max. In the future, we might choose a smoother projection on the ball to ensure that the gradient remains non zero outside the ball.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class AddBias(tf.keras.layers.Layer):\n\"\"\"Adds a bias to the input.\n\n    Remark: the euclidean norm of the bias must be bounded in advance.\n    Note that this is the euclidean norm of the whole bias vector, not\n    the norm of each element of the bias vector.\n\n    Warning: beware zero gradients outside the ball of norm norm_max.\n    In the future, we might choose a smoother projection on the ball to ensure\n    that the gradient remains non zero outside the ball.\n    \"\"\"\n\n    def __init__(self, norm_max, **kwargs):\n        super().__init__(**kwargs)\n        self.norm_max = norm_max\n\n    def build(self, input_shape):\n        self.bias = self.add_weight(\n            name=\"bias\",\n            shape=(input_shape[-1],),\n            initializer=\"zeros\",\n            trainable=True,\n        )\n\n    def call(self, inputs, **kwargs):\n        # parametrize the bias so it belongs to a ball of norm norm_max.\n        bias = tf.clip_by_norm(self.bias, self.norm_max)  # 1-Lipschitz operation.\n        return inputs + bias\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DPLayer","title":"<code>DPLayer</code>","text":"<p>Wrapper for created differentially private layers, instanciates abstract methods use for computing the bounds of the gradient relatively to the parameters and to the input.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DPLayer:\n\"\"\"\n    Wrapper for created differentially private layers, instanciates abstract methods\n    use for computing the bounds of the gradient relatively to the parameters and to the\n    input.\n    \"\"\"\n\n    @abstractmethod\n    def backpropagate_params(self, input_bound, gradient_bound):\n\"\"\"Corresponds to the Lipschitz constant of the output wrt the parameters,\n            i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.\n\n        Args:\n            input_bound: Maximum norm of input.\n            gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n        Returns:\n            Maximum norm of tangent vector.\"\"\"\n        pass\n\n    @abstractmethod\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n\"\"\"Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,\n            i.e. multiply by the Lipschitz constant of the output wrt input.\n\n        Args:\n            input_bound: Maximum norm of input.\n            gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n        Returns:\n            Maximum norm of tangent vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def propagate_inputs(self, input_bound):\n\"\"\"Maximum norm of output of element.\n\n        Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def has_parameters(self):\n        pass\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DPLayer.backpropagate_inputs","title":"<code>backpropagate_inputs(input_bound, gradient_bound)</code>  <code>abstractmethod</code>","text":"<p>Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,     i.e. multiply by the Lipschitz constant of the output wrt input.</p> <p>Parameters:</p> Name Type Description Default <code>input_bound</code> <p>Maximum norm of input.</p> required <code>gradient_bound</code> <p>Maximum norm of gradients (co-tangent vector)</p> required <p>Returns:</p> Type Description <p>Maximum norm of tangent vector.</p> Source code in <code>lipdp/layers.py</code> <pre><code>@abstractmethod\ndef backpropagate_inputs(self, input_bound, gradient_bound):\n\"\"\"Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,\n        i.e. multiply by the Lipschitz constant of the output wrt input.\n\n    Args:\n        input_bound: Maximum norm of input.\n        gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n    Returns:\n        Maximum norm of tangent vector.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DPLayer.backpropagate_params","title":"<code>backpropagate_params(input_bound, gradient_bound)</code>  <code>abstractmethod</code>","text":"<p>Corresponds to the Lipschitz constant of the output wrt the parameters,     i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.</p> <p>Parameters:</p> Name Type Description Default <code>input_bound</code> <p>Maximum norm of input.</p> required <code>gradient_bound</code> <p>Maximum norm of gradients (co-tangent vector)</p> required <p>Returns:</p> Type Description <p>Maximum norm of tangent vector.</p> Source code in <code>lipdp/layers.py</code> <pre><code>@abstractmethod\ndef backpropagate_params(self, input_bound, gradient_bound):\n\"\"\"Corresponds to the Lipschitz constant of the output wrt the parameters,\n        i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.\n\n    Args:\n        input_bound: Maximum norm of input.\n        gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n    Returns:\n        Maximum norm of tangent vector.\"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DPLayer.propagate_inputs","title":"<code>propagate_inputs(input_bound)</code>  <code>abstractmethod</code>","text":"<p>Maximum norm of output of element.</p> <p>Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.</p> Source code in <code>lipdp/layers.py</code> <pre><code>@abstractmethod\ndef propagate_inputs(self, input_bound):\n\"\"\"Maximum norm of output of element.\n\n    Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_AddBias","title":"<code>DP_AddBias</code>","text":"<p>         Bases: <code>AddBias</code>, <code>DPLayer</code></p> <p>Adds a bias to the input.</p> <p>The bias is projected on the ball of norm <code>norm_max</code> during training. The projection on the ball is a 1-Lipschitz function, since the ball is convex.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_AddBias(AddBias, DPLayer):\n\"\"\"Adds a bias to the input.\n\n    The bias is projected on the ball of norm `norm_max` during training.\n    The projection on the ball is a 1-Lipschitz function, since the ball\n    is convex.\n    \"\"\"\n\n    def __init__(self, *args, nm_coef=1, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.nm_coef = nm_coef\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        return gradient_bound  # clipping is a 1-Lipschitz operation.\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound  # adding is a 1-Lipschitz operation.\n\n    def propagate_inputs(self, input_bound):\n        return input_bound + self.norm_max\n\n    def has_parameters(self):\n        return True\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_BoundedInput","title":"<code>DP_BoundedInput</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code>, <code>DPLayer</code></p> <p>Input layer that clips the input to a given norm.</p> <p>Remark: every pipeline should start with this layer.</p> <p>Attributes:</p> Name Type Description <code>upper_bound</code> <p>Maximum norm of the input.</p> <code>enforce_clipping</code> <p>If True (default), the input is clipped to the given norm.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_BoundedInput(tf.keras.layers.Layer, DPLayer):\n\"\"\"Input layer that clips the input to a given norm.\n\n    Remark: every pipeline should start with this layer.\n\n    Attributes:\n        upper_bound: Maximum norm of the input.\n        enforce_clipping: If True (default), the input is clipped to the given norm.\n    \"\"\"\n\n    def __init__(self, *args, upper_bound, enforce_clipping=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.upper_bound = upper_bound\n        self.enforce_clipping = enforce_clipping\n\n    def call(self, x, *args, **kwargs):\n        if self.enforce_clipping:\n            axes = list(range(1, len(x.shape)))\n            x = tf.clip_by_norm(x, self.upper_bound, axes=axes)\n        return x\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"InputLayer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        if input_bound is None:\n            return self.upper_bound\n        return min(self.upper_bound, input_bound)\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_ClipGradient","title":"<code>DP_ClipGradient</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code>, <code>DPLayer</code></p> <p>Clips the gradient during the backward pass.</p> <p>Behave like identity function during the forward pass. The clipping is done automatically during the backward pass.</p> <p>Attributes:</p> Name Type Description <code>clip_value</code> <code>float</code> <p>The maximum norm of the gradient.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_ClipGradient(tf.keras.layers.Layer, DPLayer):\n\"\"\"Clips the gradient during the backward pass.\n\n    Behave like identity function during the forward pass.\n    The clipping is done automatically during the backward pass.\n\n    Attributes:\n        clip_value (float): The maximum norm of the gradient.\n    \"\"\"\n\n    def __init__(self, clip_value, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clip_value = clip_value\n\n    def call(self, inputs, *args, **kwargs):\n        batch_size = tf.cast(tf.shape(inputs)[0], tf.float32)\n        # the clipping is done elementwise\n        # since REDUCTION=SUM_OVER_BATCH_SIZE, we need to divide by batch_size\n        # to get the correct norm.\n        # this makes the clipping independent of the batch size.\n        elementwise_clip_value = self.clip_value / batch_size\n        return clip_gradient(inputs, elementwise_clip_value)\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"ClipGradient doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return min(gradient_bound, self.clip_value)\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_MaxPool2D","title":"<code>DP_MaxPool2D</code>","text":"<p>         Bases: <code>tf.keras.layers.MaxPool2D</code>, <code>DPLayer</code></p> <p>Max pooling layer that preserves the gradient norm.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_MaxPool2D(tf.keras.layers.MaxPool2D, DPLayer):\n\"\"\"Max pooling layer that preserves the gradient norm.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert (\n            self.strides is None or self.strides == self.pool_size\n        ), \"Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.\"\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"Layer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_ScaledL2NormPooling2D","title":"<code>DP_ScaledL2NormPooling2D</code>","text":"<p>         Bases: <code>lip.layers.ScaledL2NormPooling2D</code>, <code>DPLayer</code></p> <p>Max pooling layer that preserves the gradient norm.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_ScaledL2NormPooling2D(lip.layers.ScaledL2NormPooling2D, DPLayer):\n\"\"\"Max pooling layer that preserves the gradient norm.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert (\n            self.strides is None or self.strides == self.pool_size\n        ), \"Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.\"\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"Layer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_WrappedResidual","title":"<code>DP_WrappedResidual</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code>, <code>DPLayer</code></p> Source code in <code>lipdp/layers.py</code> <pre><code>class DP_WrappedResidual(tf.keras.layers.Layer, DPLayer):\n    def __init__(self, block, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.block = block\n\n    def call(self, inputs, *args, **kwargs):\n        assert len(inputs) == 2\n        i1, i2 = inputs\n        i2 = self.block(i2, *args, **kwargs)\n        return i1, i2\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        assert len(input_bound) == 2\n        assert len(gradient_bound) == 2\n        _, i2 = input_bound\n        _, g2 = gradient_bound\n        g2 = self.block.backpropagate_params(i2, g2)\n        return g2\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        assert len(input_bound) == 2\n        assert len(gradient_bound) == 2\n        _, i2 = input_bound\n        g1, g2 = gradient_bound\n        g2 = self.block.backpropagate_inputs(i2, g2)\n        return g1, g2\n\n    def propagate_inputs(self, input_bound):\n        assert len(input_bound) == 2\n        i1, i2 = input_bound\n        i2 = self.block.propagate_inputs(i2)\n        return i1, i2\n\n    def has_parameters(self):\n        return self.block.has_parameters()\n\n    @property\n    def nm_coef(self):\n\"\"\"Returns the norm multiplier coefficient of the layer.\n\n        Remark: this is a property to mimic the behavior of an attribute.\n        \"\"\"\n        return self.block.nm_coef\n</code></pre>"},{"location":"api/layers/#lipdp.layers.DP_WrappedResidual.nm_coef","title":"<code>nm_coef</code>  <code>property</code>","text":"<p>Returns the norm multiplier coefficient of the layer.</p> <p>Remark: this is a property to mimic the behavior of an attribute.</p>"},{"location":"api/layers/#lipdp.layers.DP_GNP_Factory","title":"<code>DP_GNP_Factory(layer_cls)</code>","text":"<p>Factory for creating differentially private gradient norm preserving layers that don't have parameters.</p> <p>Remark: the layer is assumed to be GNP. This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1). Pllease ensure that the layer is GNP before using this factory.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>lipdp/layers.py</code> <pre><code>def DP_GNP_Factory(layer_cls):\n\"\"\"Factory for creating differentially private gradient norm preserving layers that don't have parameters.\n\n    Remark: the layer is assumed to be GNP.\n    This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1).\n    Pllease ensure that the layer is GNP before using this factory.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    class DP_GNP(layer_cls, DPLayer):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def backpropagate_params(self, input_bound, gradient_bound):\n            raise ValueError(\"Layer doesn't have parameters\")\n\n        def backpropagate_inputs(self, input_bound, gradient_bound):\n            return 1 * gradient_bound\n\n        def propagate_inputs(self, input_bound):\n            return input_bound\n\n        def has_parameters(self):\n            return False\n\n    DP_GNP.__name__ = f\"DP_{layer_cls.__name__}\"\n    return DP_GNP\n</code></pre>"},{"location":"api/layers/#lipdp.layers.clip_gradient","title":"<code>clip_gradient(x, clip_value)</code>","text":"<p>Clips the gradient during the backward pass.</p> <p>Behave like identity function during the forward pass.</p> Source code in <code>lipdp/layers.py</code> <pre><code>@tf.custom_gradient\ndef clip_gradient(x, clip_value):\n\"\"\"Clips the gradient during the backward pass.\n\n    Behave like identity function during the forward pass.\n    \"\"\"\n\n    def grad_fn(dy):\n        # clip by norm each row\n        axes = list(range(1, len(dy.shape)))\n        clipped_dy = tf.clip_by_norm(dy, clip_value, axes=axes)\n        return clipped_dy, None  # No gradient for clip_value\n\n    return x, grad_fn\n</code></pre>"},{"location":"api/layers/#lipdp.layers.make_residuals","title":"<code>make_residuals(merge_policy, wrapped_layers)</code>","text":"<p>Returns a list of layers that implement a residual block.</p> <p>Parameters:</p> Name Type Description Default <code>merge_policy</code> <p>either \"add\" or \"1-lip-add\".</p> required <code>wrapped_layers</code> <p>a list of layers that will be wrapped in residual blocks.</p> required <p>Returns:</p> Type Description <p>A list of layers that implement a residual block.</p> Source code in <code>lipdp/layers.py</code> <pre><code>def make_residuals(merge_policy, wrapped_layers):\n\"\"\"Returns a list of layers that implement a residual block.\n\n    Args:\n        merge_policy: either \"add\" or \"1-lip-add\".\n        wrapped_layers: a list of layers that will be wrapped in residual blocks.\n\n    Returns:\n        A list of layers that implement a residual block.\n    \"\"\"\n    layers = [DP_SplitResidual()]\n\n    for layer in wrapped_layers:\n        residual_block = DP_WrappedResidual(layer)\n        layers.append(residual_block)\n\n    layers.append(DP_MergeResidual(merge_policy))\n\n    return layers\n</code></pre>"},{"location":"api/losses/","title":"lipdp.losses module","text":""},{"location":"api/losses/#lipdp.losses.DP_KCosineSimilarity","title":"<code>DP_KCosineSimilarity</code>","text":"<p>         Bases: <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_KCosineSimilarity(DP_Loss):\n    def __init__(\n        self,\n        K=1.0,\n        axis=-1,\n        reduction=tf.keras.losses.Reduction.AUTO,\n        name=\"cosine_similarity\",\n    ):\n        super().__init__(reduction=reduction, name=name)\n        # as the espilon is applied before the sqrt in tf.linalg.l2_normalize we\n        # apply square to it\n        self.K = K**2\n        self.axis = axis\n\n    @tf.function\n    def call(self, y_true, y_pred):\n        y_true = tf.linalg.l2_normalize(y_true, epsilon=self.K, axis=self.axis)\n        y_pred = tf.linalg.l2_normalize(y_pred, epsilon=self.K, axis=self.axis)\n        return -tf.reduce_sum(y_true * y_pred, axis=self.axis)\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        return 1 / float(self.K)\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_KCosineSimilarity.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    return 1 / float(self.K)\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_Loss","title":"<code>DP_Loss</code>","text":"<p>         Bases: <code>Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_Loss(Loss):\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_Loss.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MeanAbsoluteError","title":"<code>DP_MeanAbsoluteError</code>","text":"<p>         Bases: <code>tf.keras.losses.MeanAbsoluteError</code>, <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_MeanAbsoluteError(tf.keras.losses.MeanAbsoluteError, DP_Loss):\n    def __init__(\n        self,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassKR\",\n    ):\nr\"\"\"\n        Mean Absolute Error\n        \"\"\"\n        super(DP_MeanAbsoluteError, self).__init__(reduction=reduction, name=name)\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MeanAbsoluteError.__init__","title":"<code>__init__(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassKR')</code>","text":"<p>Mean Absolute Error</p> Source code in <code>lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"MulticlassKR\",\n):\nr\"\"\"\n    Mean Absolute Error\n    \"\"\"\n    super(DP_MeanAbsoluteError, self).__init__(reduction=reduction, name=name)\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MeanAbsoluteError.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHKR","title":"<code>DP_MulticlassHKR</code>","text":"<p>         Bases: <code>losses.MulticlassHKR</code>, <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_MulticlassHKR(losses.MulticlassHKR, DP_Loss):\n    def __init__(\n            self,\n            alpha=10.0,\n            min_margin=1.0,\n            reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n            name=\"MulticlassHKR\",\n    ):\n\"\"\"\n        The multiclass version of HKR. This is done by computing the HKR term over each\n        class and averaging the results.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n        pre-process the labels `y_true` with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            alpha (float): regularization factor\n            min_margin (float): margin to enforce.\n            multi_gpu (bool): set to True when running on multi-GPU/TPU\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassHKR, self).__init__(\n            alpha=alpha,\n            min_margin=min_margin,\n            multi_gpu=False,\n            reduction=reduction,\n            name=name,\n        )\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        return self.alpha + 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHKR.__init__","title":"<code>__init__(alpha=10.0, min_margin=1.0, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassHKR')</code>","text":"<p>The multiclass version of HKR. This is done by computing the HKR term over each class and averaging the results.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Using a multi-GPU/TPU strategy requires to set <code>multi_gpu</code> to True and to pre-process the labels <code>y_true</code> with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>regularization factor</p> <code>10.0</code> <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>multi_gpu</code> <code>bool</code> <p>set to True when running on multi-GPU/TPU</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassHKR'</code> Source code in <code>lipdp/losses.py</code> <pre><code>def __init__(\n        self,\n        alpha=10.0,\n        min_margin=1.0,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassHKR\",\n):\n\"\"\"\n    The multiclass version of HKR. This is done by computing the HKR term over each\n    class and averaging the results.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n    pre-process the labels `y_true` with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        alpha (float): regularization factor\n        min_margin (float): margin to enforce.\n        multi_gpu (bool): set to True when running on multi-GPU/TPU\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassHKR, self).__init__(\n        alpha=alpha,\n        min_margin=min_margin,\n        multi_gpu=False,\n        reduction=reduction,\n        name=name,\n    )\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHKR.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    return self.alpha + 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHinge","title":"<code>DP_MulticlassHinge</code>","text":"<p>         Bases: <code>losses.MulticlassHinge</code>, <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_MulticlassHinge(losses.MulticlassHinge, DP_Loss):\n    def __init__(\n            self,\n            min_margin=1.0,\n            reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n            name=\"MulticlassHinge\",\n    ):\n\"\"\"\n        Loss to estimate the Hinge loss in a multiclass setup. It computes the\n        element-wise Hinge term. Note that this formulation differs from the one\n        commonly found in tensorflow/pytorch (which maximises the difference between\n        the two largest logits). This formulation is consistent with the binary\n        classification loss used in a multiclass fashion.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            min_margin (float): margin to enforce.\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassHinge, self).__init__(\n            min_margin=min_margin, reduction=reduction, name=name\n        )\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHinge.__init__","title":"<code>__init__(min_margin=1.0, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassHinge')</code>","text":"<p>Loss to estimate the Hinge loss in a multiclass setup. It computes the element-wise Hinge term. Note that this formulation differs from the one commonly found in tensorflow/pytorch (which maximises the difference between the two largest logits). This formulation is consistent with the binary classification loss used in a multiclass fashion.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassHinge'</code> Source code in <code>lipdp/losses.py</code> <pre><code>def __init__(\n        self,\n        min_margin=1.0,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassHinge\",\n):\n\"\"\"\n    Loss to estimate the Hinge loss in a multiclass setup. It computes the\n    element-wise Hinge term. Note that this formulation differs from the one\n    commonly found in tensorflow/pytorch (which maximises the difference between\n    the two largest logits). This formulation is consistent with the binary\n    classification loss used in a multiclass fashion.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        min_margin (float): margin to enforce.\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassHinge, self).__init__(\n        min_margin=min_margin, reduction=reduction, name=name\n    )\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassHinge.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassKR","title":"<code>DP_MulticlassKR</code>","text":"<p>         Bases: <code>losses.MulticlassKR</code>, <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_MulticlassKR(losses.MulticlassKR, DP_Loss):\n    def __init__(\n            self,\n            reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n            name=\"MulticlassKR\",\n    ):\nr\"\"\"\n        Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein\n        duality over outputs. In this multiclass setup, the KR term is computed for each\n        class and then averaged.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n        pre-process the labels `y_true` with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            multi_gpu (bool): set to True when running on multi-GPU/TPU\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassKR, self).__init__(reduction=reduction, name=name)\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassKR.__init__","title":"<code>__init__(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassKR')</code>","text":"<p>Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein duality over outputs. In this multiclass setup, the KR term is computed for each class and then averaged.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Using a multi-GPU/TPU strategy requires to set <code>multi_gpu</code> to True and to pre-process the labels <code>y_true</code> with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>multi_gpu</code> <code>bool</code> <p>set to True when running on multi-GPU/TPU</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassKR'</code> Source code in <code>lipdp/losses.py</code> <pre><code>def __init__(\n        self,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassKR\",\n):\nr\"\"\"\n    Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein\n    duality over outputs. In this multiclass setup, the KR term is computed for each\n    class and then averaged.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n    pre-process the labels `y_true` with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        multi_gpu (bool): set to True when running on multi-GPU/TPU\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassKR, self).__init__(reduction=reduction, name=name)\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_MulticlassKR.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_TauCategoricalCrossentropy","title":"<code>DP_TauCategoricalCrossentropy</code>","text":"<p>         Bases: <code>losses.TauCategoricalCrossentropy</code>, <code>DP_Loss</code></p> Source code in <code>lipdp/losses.py</code> <pre><code>class DP_TauCategoricalCrossentropy(losses.TauCategoricalCrossentropy, DP_Loss):\n    def __init__(\n            self,\n            tau,\n            reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n            name=\"TauCategoricalCrossentropy\",\n    ):\n\"\"\"\n        Similar to original categorical crossentropy, but with a settable temperature\n        parameter.\n\n        Args:\n            tau (float): temperature parameter.\n            reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n            name (str): name of the loss\n        \"\"\"\n        super(DP_TauCategoricalCrossentropy, self).__init__(\n            tau=tau, reduction=reduction, name=name\n        )\n\n    def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n        # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n        return math.sqrt(2)\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_TauCategoricalCrossentropy.__init__","title":"<code>__init__(tau, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='TauCategoricalCrossentropy')</code>","text":"<p>Similar to original categorical crossentropy, but with a settable temperature parameter.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>temperature parameter.</p> required <code>reduction</code> <p>reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.</p> <code>tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>name of the loss</p> <code>'TauCategoricalCrossentropy'</code> Source code in <code>lipdp/losses.py</code> <pre><code>def __init__(\n        self,\n        tau,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"TauCategoricalCrossentropy\",\n):\n\"\"\"\n    Similar to original categorical crossentropy, but with a settable temperature\n    parameter.\n\n    Args:\n        tau (float): temperature parameter.\n        reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n        name (str): name of the loss\n    \"\"\"\n    super(DP_TauCategoricalCrossentropy, self).__init__(\n        tau=tau, reduction=reduction, name=name\n    )\n</code></pre>"},{"location":"api/losses/#lipdp.losses.DP_TauCategoricalCrossentropy.get_L","title":"<code>get_L()</code>","text":"<p>returns the lipschitz constant of the loss</p> Source code in <code>lipdp/losses.py</code> <pre><code>def get_L(self):\n\"\"\"returns the lipschitz constant of the loss\"\"\"\n    # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n    return math.sqrt(2)\n</code></pre>"},{"location":"api/model/","title":"lipdp.model module","text":""},{"location":"api/model/#lipdp.model.DP_Accountant","title":"<code>DP_Accountant</code>","text":"<p>         Bases: <code>keras.callbacks.Callback</code></p> <p>Callback to compute the DP guarantees at the end of each epoch.</p> <p>Note: wandb is not a strict requirement for this callback to work, logging is also supported.</p> <p>Attributes:</p> Name Type Description <code>log_fn</code> <p>log function to use. Takes a dictionary of (key, value) pairs as input.     if 'wandb', use wandb.log.     if 'logging', use logging.info.     if 'all', use both wandb.log and logging.info.</p> Source code in <code>lipdp/model.py</code> <pre><code>class DP_Accountant(keras.callbacks.Callback):\n\"\"\"Callback to compute the DP guarantees at the end of each epoch.\n\n    Note: wandb is not a strict requirement for this callback to work, logging is also supported.\n\n    Attributes:\n        log_fn: log function to use. Takes a dictionary of (key, value) pairs as input.\n                if 'wandb', use wandb.log.\n                if 'logging', use logging.info.\n                if 'all', use both wandb.log and logging.info.\n    \"\"\"\n\n    def __init__(self, log_fn=\"all\"):\n        super().__init__()\n        if log_fn == \"wandb\":\n            import wandb\n\n            log_fn = wandb.log\n        elif log_fn == \"logging\":\n            import logging\n\n            log_fn = logging.info\n        elif log_fn == \"all\":\n            import wandb\n            import logging\n\n            log_fn = lambda x: [wandb.log(x), logging.info(x)]\n        self.log_fn = log_fn\n\n    def on_epoch_end(self, epoch, logs=None):\n        niter = (epoch + 1) * self.model.dataset_metadata.nb_steps_per_epochs\n        epsilon, delta = get_eps_delta(model=self.model, niter=niter)\n        print(f\"\\n {(epsilon,delta)}-DP guarantees for epoch {epoch+1} \\n\")\n        # plot epoch at the same time as epsilon and delta to ease comparison of plots in wandb API.\n        self.log_fn({\"epsilon\": epsilon, \"delta\": delta, \"epoch\": epoch + 1})\n</code></pre>"},{"location":"api/model/#lipdp.model.DP_Model","title":"<code>DP_Model</code>","text":"<p>         Bases: <code>lip.model.Model</code></p> Source code in <code>lipdp/model.py</code> <pre><code>class DP_Model(lip.model.Model):\n    def __init__(\n            self,\n            dp_layers,\n            *args,\n            dp_parameters: DPParameters,\n            dataset_metadata: DatasetMetadata,\n            debug: bool = False,\n            **kwargs,\n    ):\n\"\"\"Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since\n        the framework assume Lipschitz layers.\n\n        Args:\n            dp_layers: the list of layers to use ( as done in sequential ) but here we can leverage\n                the fact that layers may have multiple inputs/outputs.\n            dp_parameters (DPParameters): parameters used to set the dp procedure.\n            dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n                of training samples, the input bound, number of batches in the dataset and the batch size.\n            debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n        Note:\n            The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n            DP accounting is done with the associated Callback.\n\n        Raises:\n            TypeError: when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.dp_layers = dp_layers\n        self.dp_parameters = dp_parameters\n        self.dataset_metadata = dataset_metadata\n        self.debug = debug\n        if self.dp_parameters.noisify_strategy == \"global\":\n            self.noisify_fun = global_noisify\n        elif self.dp_parameters.noisify_strategy == \"local\":\n            self.noisify_fun = local_noisify\n        else:\n            raise TypeError(\n                \"Incorrect noisify_strategy argument during model initialisation.\"\n            )\n\n    def layers_forward_order(self):\n        return self.dp_layers\n\n    def layers_backward_order(self):\n        return self.dp_layers[::-1]\n\n    def call(self, inputs, *args, **kwarsg):\n        x = inputs\n        for layer in self.layers_forward_order():\n            x = layer(x, *args, **kwarsg)\n        return x\n\n    # Define the differentially private training step\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # tf.cast(y_pred,dtype=y.dtype)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Get gradient bounds\n        gradient_bounds = compute_gradient_bounds(model=self)\n        noisy_gradients = self.noisify_fun(\n            self, gradient_bounds, trainable_vars, gradients\n        )\n        # Each optimizer is a postprocessing of the already (epsilon,delta)-DP gradients\n        self.optimizer.apply_gradients(zip(noisy_gradients, trainable_vars))\n        # self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update Metrics\n        self.compiled_metrics.update_state(y, y_pred)\n        # Condense to verify |W|_2 = 1\n        self.condense()\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>"},{"location":"api/model/#lipdp.model.DP_Model.__init__","title":"<code>__init__(dp_layers, *args, dp_parameters, dataset_metadata, debug=False, **kwargs)</code>","text":"<p>Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since the framework assume Lipschitz layers.</p> <p>Parameters:</p> Name Type Description Default <code>dp_layers</code> <p>the list of layers to use ( as done in sequential ) but here we can leverage the fact that layers may have multiple inputs/outputs.</p> required <code>dp_parameters</code> <code>DPParameters</code> <p>parameters used to set the dp procedure.</p> required <code>dataset_metadata</code> <code>DatasetMetadata</code> <p>information about the dataset. Must contain: the input shape, number of training samples, the input bound, number of batches in the dataset and the batch size.</p> required <code>debug</code> <code>bool</code> <p>when true print additionnal debug informations (must be in eager mode). Defaults to False.</p> <code>False</code> Note <p>The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step. DP accounting is done with the associated Callback.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"</p> Source code in <code>lipdp/model.py</code> <pre><code>def __init__(\n        self,\n        dp_layers,\n        *args,\n        dp_parameters: DPParameters,\n        dataset_metadata: DatasetMetadata,\n        debug: bool = False,\n        **kwargs,\n):\n\"\"\"Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since\n    the framework assume Lipschitz layers.\n\n    Args:\n        dp_layers: the list of layers to use ( as done in sequential ) but here we can leverage\n            the fact that layers may have multiple inputs/outputs.\n        dp_parameters (DPParameters): parameters used to set the dp procedure.\n        dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n            of training samples, the input bound, number of batches in the dataset and the batch size.\n        debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n    Note:\n        The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n        DP accounting is done with the associated Callback.\n\n    Raises:\n        TypeError: when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.dp_layers = dp_layers\n    self.dp_parameters = dp_parameters\n    self.dataset_metadata = dataset_metadata\n    self.debug = debug\n    if self.dp_parameters.noisify_strategy == \"global\":\n        self.noisify_fun = global_noisify\n    elif self.dp_parameters.noisify_strategy == \"local\":\n        self.noisify_fun = local_noisify\n    else:\n        raise TypeError(\n            \"Incorrect noisify_strategy argument during model initialisation.\"\n        )\n</code></pre>"},{"location":"api/model/#lipdp.model.DP_Sequential","title":"<code>DP_Sequential</code>","text":"<p>         Bases: <code>lip.model.Sequential</code></p> Source code in <code>lipdp/model.py</code> <pre><code>class DP_Sequential(lip.model.Sequential):\n    def __init__(\n            self,\n            *args,\n            dp_parameters: DPParameters,\n            dataset_metadata: DatasetMetadata,\n            debug: bool = False,\n            **kwargs,\n    ):\n\"\"\"Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since\n        the framework assume Lipschitz layers.\n\n        Args:\n            dp_parameters (DPParameters): parameters used to set the dp procedure.\n            dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n                of training samples, the input bound, number of batches in the dataset and the batch size.\n            debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n        Note:\n            The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n            DP accounting is done with the associated Callback.\n\n        Raises:\n            TypeError: when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.dp_parameters = dp_parameters\n        self.dataset_metadata = dataset_metadata\n        self.debug = debug\n        if self.dp_parameters.noisify_strategy == \"global\":\n            self.noisify_fun = global_noisify\n        elif self.dp_parameters.noisify_strategy == \"local\":\n            self.noisify_fun = local_noisify\n        else:\n            raise TypeError(\n                \"Incorrect noisify_strategy argument during model initialisation.\"\n            )\n\n    def layers_forward_order(self):\n        return self.layers\n\n    def layers_backward_order(self):\n        return self.layers[::-1]\n\n    # Define the differentially private training step\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # tf.cast(y_pred,dtype=y.dtype)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Get gradient bounds\n        gradient_bounds = compute_gradient_bounds(model=self)\n        noisy_gradients = self.noisify_fun(\n            self, gradient_bounds, trainable_vars, gradients\n        )\n        # Each optimizer is a postprocessing of the already (epsilon,delta)-DP gradients\n        self.optimizer.apply_gradients(zip(noisy_gradients, trainable_vars))\n        # self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update Metrics\n        self.compiled_metrics.update_state(y, y_pred)\n        # Condense to verify |W|_2 = 1\n        self.condense()\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>"},{"location":"api/model/#lipdp.model.DP_Sequential.__init__","title":"<code>__init__(*args, dp_parameters, dataset_metadata, debug=False, **kwargs)</code>","text":"<p>Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since the framework assume Lipschitz layers.</p> <p>Parameters:</p> Name Type Description Default <code>dp_parameters</code> <code>DPParameters</code> <p>parameters used to set the dp procedure.</p> required <code>dataset_metadata</code> <code>DatasetMetadata</code> <p>information about the dataset. Must contain: the input shape, number of training samples, the input bound, number of batches in the dataset and the batch size.</p> required <code>debug</code> <code>bool</code> <p>when true print additionnal debug informations (must be in eager mode). Defaults to False.</p> <code>False</code> Note <p>The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step. DP accounting is done with the associated Callback.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"</p> Source code in <code>lipdp/model.py</code> <pre><code>def __init__(\n        self,\n        *args,\n        dp_parameters: DPParameters,\n        dataset_metadata: DatasetMetadata,\n        debug: bool = False,\n        **kwargs,\n):\n\"\"\"Model Class based on the deel.lip.Sequential model. Only layer from the lipdp.layers module are allowed since\n    the framework assume Lipschitz layers.\n\n    Args:\n        dp_parameters (DPParameters): parameters used to set the dp procedure.\n        dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n            of training samples, the input bound, number of batches in the dataset and the batch size.\n        debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n    Note:\n        The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n        DP accounting is done with the associated Callback.\n\n    Raises:\n        TypeError: when the dp_parameters.noisify_strategy is not one of \"local\" or \"global\"\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.dp_parameters = dp_parameters\n    self.dataset_metadata = dataset_metadata\n    self.debug = debug\n    if self.dp_parameters.noisify_strategy == \"global\":\n        self.noisify_fun = global_noisify\n    elif self.dp_parameters.noisify_strategy == \"local\":\n        self.noisify_fun = local_noisify\n    else:\n        raise TypeError(\n            \"Incorrect noisify_strategy argument during model initialisation.\"\n        )\n</code></pre>"},{"location":"api/model/#lipdp.model.compute_gradient_bounds","title":"<code>compute_gradient_bounds(model)</code>","text":"<p>Compute the gradient norm bounds of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <p>Returns:</p> Name Type Description <code>gradient_bounds</code> <p>dictionary of gradient norm bounds with (key, value) pairs (layer_name, gradient_bound).              The order of the bounds is the same as              the order of the layers returned by model.layers_backward_order().</p> Source code in <code>lipdp/model.py</code> <pre><code>def compute_gradient_bounds(model):\n\"\"\"Compute the gradient norm bounds of the model.\n\n    Args:\n        model: model to train.\n\n    Returns:\n        gradient_bounds: dictionary of gradient norm bounds with (key, value) pairs (layer_name, gradient_bound).\n                         The order of the bounds is the same as\n                         the order of the layers returned by model.layers_backward_order().\n    \"\"\"\n    # Initialisation, get lipschitz constant of loss\n    input_bounds = {}\n    gradient_bounds = {}\n    input_bound = None  # Unknown at the start.\n\n    # Forward pass to assess maximum activation norms\n    for layer in model.layers_forward_order():\n        assert isinstance(layer, DPLayer)\n        if model.debug:\n            print(f\"Layer {layer.name} input bound: {input_bound}\")\n        input_bounds[layer.name] = input_bound\n        input_bound = layer.propagate_inputs(input_bound)\n\n    if model.debug:\n        print(f\"Layer {layer.name} input bound: {input_bound}\")\n\n    # since we aggregate using SUM_OVER_BATCH\n    gradient_bound = model.loss.get_L() / model.dataset_metadata.batch_size\n\n    # Backward pass to compute gradient norm bounds and accumulate Lip constant\n    for layer in model.layers_backward_order():\n        assert isinstance(layer, DPLayer)\n        layer_input_bound = input_bounds[layer.name]\n        if layer.has_parameters():\n            assert len(layer.trainable_variables) == 1\n            var_name = layer.trainable_variables[0].name\n            gradient_bounds[var_name] = layer.backpropagate_params(\n                layer_input_bound, gradient_bound\n            )\n        gradient_bound = layer.backpropagate_inputs(layer_input_bound, gradient_bound)\n\n    # Return gradient bounds\n    return gradient_bounds\n</code></pre>"},{"location":"api/model/#lipdp.model.get_noise_multiplier_coefs","title":"<code>get_noise_multiplier_coefs(model)</code>","text":"<p>Get the noise multiplier coefficients of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <p>Returns:</p> Name Type Description <code>dict_coefs</code> <p>dictionary of noise multiplier coefficients.         The order of the coefficients is the same as         the order of the layers returned by model.layers_forward_order().</p> Source code in <code>lipdp/model.py</code> <pre><code>def get_noise_multiplier_coefs(model):\n\"\"\"Get the noise multiplier coefficients of the model.\n\n    Args:\n        model: model to train.\n\n    Returns:\n        dict_coefs: dictionary of noise multiplier coefficients.\n                    The order of the coefficients is the same as\n                    the order of the layers returned by model.layers_forward_order().\n    \"\"\"\n    dict_coefs = {}\n    for (\n        layer\n    ) in (\n        model.layers_forward_order()\n    ):  # remark: insertion order is preserved in Python 3.7+\n        assert isinstance(layer, DPLayer)\n        if layer.has_parameters():\n            assert len(layer.trainable_variables) == 1\n            dict_coefs[layer.trainable_variables[0].name] = layer.nm_coef\n    return dict_coefs\n</code></pre>"},{"location":"api/model/#lipdp.model.global_noisify","title":"<code>global_noisify(model, gradient_bounds, trainable_vars, gradients)</code>","text":"<p>Add noise to gradients.</p> a single global noise is added to all gradients, based on the global sensitivity. <p>This is the default behaviour of the original DPGD algorithm. This may yield looser privacy bounds than local noisify.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <code>gradient_bounds</code> <p>dictionary of gradient norm upper bounds. The keys are the names of the trainable variables.</p> required <code>trainable_vars</code> <p>list of trainable variables. The list is in the same order as gradients.</p> required <code>gradients</code> <p>list of gradients to add noise to. The list is in the same order as trainable_vars.</p> required <p>Returns:</p> Name Type Description <code>noisy_grads</code> <p>list of noisy gradients. The list is in the same order as trainable_vars.</p> Source code in <code>lipdp/model.py</code> <pre><code>def global_noisify(model, gradient_bounds, trainable_vars, gradients):\n\"\"\"Add noise to gradients.\n\n    Remark: a single global noise is added to all gradients, based on the global sensitivity.\n            This is the default behaviour of the original DPGD algorithm.\n            This may yield looser privacy bounds than local noisify.\n\n    Args:\n        model: model to train.\n        gradient_bounds: dictionary of gradient norm upper bounds. The keys are the names of the trainable variables.\n        trainable_vars: list of trainable variables. The list is in the same order as gradients.\n        gradients: list of gradients to add noise to. The list is in the same order as trainable_vars.\n\n    Returns:\n        noisy_grads: list of noisy gradients. The list is in the same order as trainable_vars.\n    \"\"\"\n    global_sensitivity = np.sqrt(\n        sum([bound**2 for bound in gradient_bounds.values()])\n    )\n    stddev = model.dp_parameters.noise_multiplier * global_sensitivity\n    noises = [tf.random.normal(shape=tf.shape(g), stddev=stddev) for g in gradients]\n    if model.debug:\n        for grad, var in zip(gradients, trainable_vars):\n            upperboundgrad = (\n                gradient_bounds[var.name] * model.dataset_metadata.batch_size\n            )\n            noise_msg = (\n                f\"Adding noise of stddev : {stddev}\"\n                f\" to variable {var.name}\"\n                f\" of gradient norm upper bound {upperboundgrad}\"\n                f\" and effective norm {tf.norm(grad)}\"\n            )\n            print(noise_msg)\n    noisy_grads = [g + n for g, n in zip(gradients, noises)]\n    return noisy_grads\n</code></pre>"},{"location":"api/model/#lipdp.model.local_noisify","title":"<code>local_noisify(model, gradient_bounds, trainable_vars, gradients)</code>","text":"<p>Add noise to gradients of trainable variables.</p> <p>Remark: this yields tighter bounds than global_noisify.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <code>gradient_bounds</code> <p>dictionary of gradient norm upper bounds. Keys are trainable variables names.</p> required <code>trainable_vars</code> <p>list of trainable variables. Same order as gradients.</p> required <code>gradients</code> <p>list of gradients. Same order as trainable_vars.</p> required <p>Returns:</p> Type Description <p>list of noisy gradients. Same order as trainable_vars.</p> Source code in <code>lipdp/model.py</code> <pre><code>def local_noisify(model, gradient_bounds, trainable_vars, gradients):\n\"\"\"Add noise to gradients of trainable variables.\n\n    Remark: this yields tighter bounds than global_noisify.\n\n    Args:\n        model: model to train.\n        gradient_bounds: dictionary of gradient norm upper bounds. Keys are trainable variables names.\n        trainable_vars: list of trainable variables. Same order as gradients.\n        gradients: list of gradients. Same order as trainable_vars.\n\n    Returns:\n        list of noisy gradients. Same order as trainable_vars.\n    \"\"\"\n    nm_coefs = get_noise_multiplier_coefs(model)\n    noises = []\n    for grad, var in zip(gradients, trainable_vars):\n        if var.name in gradient_bounds.keys():\n            stddev = (\n                model.dp_parameters.noise_multiplier\n                * gradient_bounds[var.name]\n                * nm_coefs[var.name]\n                * 2\n            )\n            noises.append(tf.random.normal(shape=tf.shape(grad), stddev=stddev))\n            if model.debug:\n                upperboundgrad = (\n                    gradient_bounds[var.name] * model.dataset_metadata.batch_size\n                )\n                noise_msg = (\n                    f\"Adding noise of stddev : {stddev}\"\n                    f\" to variable {var.name}\"\n                    f\" of gradient norm upper bound {upperboundgrad}\"\n                    f\" and effective norm {tf.norm(grad)}\"\n                )\n                print(noise_msg)\n        else:\n            raise ValueError(f\"Variable {var.name} not in gradient bounds.\")\n\n    noisy_grads = [g + n for g, n in zip(gradients, noises)]\n    return noisy_grads\n</code></pre>"},{"location":"api/pipeline/","title":"lipdp.pipeline module","text":""},{"location":"api/pipeline/#lipdp.pipeline.DatasetMetadata","title":"<code>DatasetMetadata</code>  <code>dataclass</code>","text":"<p>class that handle dataset metadata that will be used to compute privacy guarantees</p> Source code in <code>lipdp/pipeline.py</code> <pre><code>@dataclass\nclass DatasetMetadata:\n\"\"\"\n    class that handle dataset metadata that will be used\n    to compute privacy guarantees\n    \"\"\"\n\n    input_shape: Tuple[int, int, int]\n    nb_classes: int\n    nb_samples_train: int\n    nb_samples_test: int\n    class_names: List[str]\n    nb_steps_per_epochs: int\n    batch_size: int\n    max_norm: float\n</code></pre>"},{"location":"api/pipeline/#lipdp.pipeline.load_and_prepare_data","title":"<code>load_and_prepare_data(dataset_name='mnist', batch_size=256, colorspace='RGB', drop_remainder=True, augmentation_fct=None, bound_fct=None)</code>","text":"<p>load dataset_name data using tensorflow datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset to load.</p> <code>'mnist'</code> <code>batch_size</code> <code>int</code> <p>batch size</p> <code>256</code> <code>colorspace</code> <code>str</code> <p>one of RGB, HSV, YIQ, YUV</p> <code>'RGB'</code> <code>drop_remainder</code> <code>bool</code> <p>when true drop the last batch if it has less than batch_size elements. Defaults to True.</p> <code>True</code> <code>augmentation_fct</code> <code>callable</code> <p>data augmentation to be applied to train. the function must take a tuple (img, label) and return a tuple of (img, label). Defaults to None.</p> <code>None</code> <code>bound_fct</code> <code>callable</code> <p>function that is responsible of bounding the inputs. Can be None, bound_normalize or bound_clip_value. None means that no clipping is performed, and max theoretical value is reported ( sqrt(whc) ). bound_normalize means that each input is normalized setting the bound to 1. bound_clip_value will clip norm to defined value.</p> <code>None</code> <p>Returns:</p> Type Description <p>ds_train, ds_test, metadat: two dataset, with data preparation, augmentation, shuffling and batching. Also return an DatasetMetadata object with infos about the dataset.</p> Source code in <code>lipdp/pipeline.py</code> <pre><code>def load_and_prepare_data(\n    dataset_name: str = \"mnist\",\n    batch_size: int = 256,\n    colorspace: str = \"RGB\",\n    drop_remainder=True,\n    augmentation_fct=None,\n    bound_fct=None,\n):\n\"\"\"\n    load dataset_name data using tensorflow datasets.\n\n    Args:\n        dataset_name (str): name of the dataset to load.\n        batch_size (int): batch size\n        colorspace (str): one of RGB, HSV, YIQ, YUV\n        drop_remainder (bool, optional): when true drop the last batch if it\n            has less than batch_size elements. Defaults to True.\n        augmentation_fct (callable, optional): data augmentation to be applied\n            to train. the function must take a tuple (img, label) and return a\n            tuple of (img, label). Defaults to None.\n        bound_fct (callable, optional): function that is responsible of\n            bounding the inputs. Can be None, bound_normalize or bound_clip_value.\n            None means that no clipping is performed, and max theoretical value is\n            reported ( sqrt(w*h*c) ). bound_normalize means that each input is\n            normalized setting the bound to 1. bound_clip_value will clip norm to\n            defined value.\n\n    Returns:\n        ds_train, ds_test, metadat: two dataset, with data preparation,\n            augmentation, shuffling and batching. Also return an\n            DatasetMetadata object with infos about the dataset.\n    \"\"\"\n    # load data\n    (ds_train, ds_test), ds_info = tfds.load(\n        dataset_name,\n        split=[\"train\", \"test\"],\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n    )\n    # handle case where functions are None\n    if augmentation_fct is None:\n        augmentation_fct = lambda x, y: (x, y)\n    # None bound yield default trivial bound\n    nb_classes = ds_info.features[\"label\"].num_classes\n    input_shape = ds_info.features[\"image\"].shape\n    if bound_fct is None:\n        bound_fct = (\n            lambda x, y: (x, y),\n            input_shape[-3] * input_shape[-2] * input_shape[-1],\n        )\n    bound_callable, bound_val = bound_fct\n    # train pipeline\n    ds_train = (\n        ds_train.map(  # map to 0,1 and one hot encode\n            lambda x, y: (\n                tf.cast(x, tf.float32) / 255.0,\n                tf.one_hot(y, nb_classes),\n            ),\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .shuffle(  # shuffle\n            min(batch_size * 10, max(batch_size, ds_train.cardinality())),\n            reshuffle_each_iteration=True,\n        )\n        .map(augmentation_fct, num_parallel_calls=tf.data.AUTOTUNE)  # augment\n        .map(  # map colorspace\n            get_colorspace_function(colorspace),\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .map(bound_callable, num_parallel_calls=tf.data.AUTOTUNE)  # apply bound\n        .batch(batch_size, drop_remainder=drop_remainder)  # batch\n        .prefetch(tf.data.AUTOTUNE)\n    )\n\n    ds_test = (\n        ds_test.map(\n            lambda x, y: (\n                tf.cast(x, tf.float32) / 255.0,\n                tf.one_hot(y, nb_classes),\n            ),\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .map(\n            get_colorspace_function(colorspace),\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .shuffle(\n            min(batch_size * 10, max(batch_size, ds_test.cardinality())),\n            reshuffle_each_iteration=True,\n        )\n        .batch(batch_size, drop_remainder=drop_remainder)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n    # get dataset metadata\n    metadata = DatasetMetadata(\n        input_shape=ds_info.features[\"image\"].shape,\n        nb_classes=ds_info.features[\"label\"].num_classes,\n        nb_samples_train=ds_info.splits[\"train\"].num_examples,\n        nb_samples_test=ds_info.splits[\"test\"].num_examples,\n        class_names=ds_info.features[\"label\"].names,\n        nb_steps_per_epochs=ds_train.cardinality().numpy()\n        if ds_train.cardinality() &gt; 0  # handle case cardinality return -1 (unknown)\n        else ds_info.splits[\"train\"].num_examples / batch_size,\n        batch_size=batch_size,\n        max_norm=bound_val,\n    )\n\n    return ds_train, ds_test, metadata\n</code></pre>"},{"location":"api/sensitivity/","title":"lipdp.sensitivity module","text":""},{"location":"api/sensitivity/#lipdp.sensitivity.get_max_epochs","title":"<code>get_max_epochs(epsilon_max, model, epochs_max=1024)</code>","text":"<p>Return the maximum number of epochs to reach a given epsilon_max value.</p> <p>The computation of (epsilon, delta) is slow since it involves solving a minimization problem (mandatory to go from RDP accountant to DP results). Hence each call takes typically around 1s. This function is used to avoid calling get_eps_delta too many times be leveraging the fact that epsilon is a non-decreasing function of the number of epochs: we unlocks the dichotomy search.</p> <p>Hence the number of calls is typically log2(epochs_max) + 1. The maximum of epochs is set to 1024 by default to avoid too long computation times, even in high privacy regimes.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon_max</code> <p>The maximum value of epsilon we want to reach.</p> required <code>model</code> <p>The model used to compute the values of epsilon.</p> required <code>epochs_max</code> <p>The maximum number of epochs to reach epsilon_max. Defaults to 1024.         If None, the dichotomy search is used to find the upper bound.</p> <code>1024</code> <p>Returns:</p> Type Description <p>The maximum number of epochs to reach epsilon_max.</p> Source code in <code>lipdp/sensitivity.py</code> <pre><code>def get_max_epochs(epsilon_max, model, epochs_max=1024):\n\"\"\"Return the maximum number of epochs to reach a given epsilon_max value.\n\n    The computation of (epsilon, delta) is slow since it involves solving a minimization problem\n    (mandatory to go from RDP accountant to DP results). Hence each call takes typically around 1s.\n    This function is used to avoid calling get_eps_delta too many times be leveraging the fact that\n    epsilon is a non-decreasing function of the number of epochs: we unlocks the dichotomy search.\n\n    Hence the number of calls is typically log2(epochs_max) + 1.\n    The maximum of epochs is set to 1024 by default to avoid too long computation times, even in high\n    privacy regimes.\n\n    Args:\n        epsilon_max: The maximum value of epsilon we want to reach.\n        model: The model used to compute the values of epsilon.\n        epochs_max: The maximum number of epochs to reach epsilon_max. Defaults to 1024.\n                    If None, the dichotomy search is used to find the upper bound.\n\n    Returns:\n        The maximum number of epochs to reach epsilon_max.\"\"\"\n    steps_per_epoch = model.dataset_metadata.nb_steps_per_epochs\n\n    def fun(epoch):\n        if epoch == 0:\n            epsilon = 0\n        else:\n            epoch = round(epoch)\n            niter = (epoch + 1) * steps_per_epoch\n            epsilon, _ = get_eps_delta(model, niter)\n        return epsilon\n\n    # dichotomy search on the number of epochs.\n    if epochs_max is None:\n        epochs_max = 512\n        epsilon = 0\n        while epsilon &lt; epsilon_max:\n            epochs_max *= 2\n            epsilon = fun(epochs_max)\n            print(f\"epochs_max = {epochs_max} at epsilon = {epsilon}\")\n\n    epochs_min = 0\n\n    while epochs_max - epochs_min &gt; 1:\n        epoch = (epochs_max + epochs_min) / 2\n        epsilon = fun(epoch)\n        if epsilon &lt; epsilon_max:\n            epochs_min = epoch\n        else:\n            epochs_max = epoch\n        print(\n            f\"epoch bounds = {epochs_min, epochs_max} and epsilon = {epsilon} at epoch {epoch}\"\n        )\n\n    return int(round(epoch))\n</code></pre>"},{"location":"api/sensitivity/#lipdp.sensitivity.gradient_norm_check","title":"<code>gradient_norm_check(K_list, model, examples)</code>","text":"<p>Verifies that the values of per-sample gradients on a layer never exceede a theoretical value determined by our theoretical work.</p> Args <p>Klist: The list of theoretical upper bounds we have identified for each layer and want to put to the test. model: The model containing the layers we are interested in. Layers must only have one trainable variable. Model must have a given input_shape or has to be built. examples: Relevant examples. Inputting the whole training set might prove very costly to check element wise Jacobians.</p> Returns <p>Boolean value. True corresponds to upper bound has been validated.</p> Source code in <code>lipdp/sensitivity.py</code> <pre><code>def gradient_norm_check(K_list, model, examples):\n\"\"\"\n    Verifies that the values of per-sample gradients on a layer never exceede a theoretical value\n    determined by our theoretical work.\n    Args :\n        Klist: The list of theoretical upper bounds we have identified for each layer and want to\n        put to the test.\n        model: The model containing the layers we are interested in. Layers must only have one trainable variable.\n        Model must have a given input_shape or has to be built.\n        examples: Relevant examples. Inputting the whole training set might prove very costly to check element wise Jacobians.\n    Returns :\n        Boolean value. True corresponds to upper bound has been validated.\n    \"\"\"\n    image_axes = tuple(range(1, examples.ndim))\n    example_norms = tf.math.reduce_euclidean_norm(examples, axis=image_axes)\n    X_max = tf.reduce_max(example_norms).numpy()\n    upper_bounds = np.array(K_list) * X_max\n    assert len(model.layers) == len(upper_bounds)\n    for layer, bound in zip(model.layers, upper_bounds):\n        assert check_layer_gradient_norm(bound, layer, examples)\n</code></pre>"}]}