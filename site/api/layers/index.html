
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../losses/">
      
      <link rel="icon" href="../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.14">
    
    
      
        <title>lipdp.layers module - lipdp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.85bb2934.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a6bdf11c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/custom.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lipdp.layers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="lipdp" class="md-header__button md-logo" aria-label="lipdp" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            lipdp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              lipdp.layers module
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
            
              <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
              </label>
            
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
            
              <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
              </label>
            
          
        </form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/anonymized-ai/lipdp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    anonymized-ai/lipdp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="lipdp" class="md-nav__button md-logo" aria-label="lipdp" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    lipdp
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/anonymized-ai/lipdp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    anonymized-ai/lipdp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          API Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          lipdp.layers module
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        lipdp.layers module
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers" class="md-nav__link">
    lipdp.layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.AddBias" class="md-nav__link">
    AddBias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer" class="md-nav__link">
    DPLayer
  </a>
  
    <nav class="md-nav" aria-label="DPLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.backpropagate_inputs" class="md-nav__link">
    backpropagate_inputs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.backpropagate_params" class="md-nav__link">
    backpropagate_params()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.propagate_inputs" class="md-nav__link">
    propagate_inputs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_AddBias" class="md-nav__link">
    DP_AddBias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_BoundedInput" class="md-nav__link">
    DP_BoundedInput
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_ClipGradient" class="md-nav__link">
    DP_ClipGradient
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_MaxPool2D" class="md-nav__link">
    DP_MaxPool2D
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_ScaledL2NormPooling2D" class="md-nav__link">
    DP_ScaledL2NormPooling2D
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_WrappedResidual" class="md-nav__link">
    DP_WrappedResidual
  </a>
  
    <nav class="md-nav" aria-label="DP_WrappedResidual">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DP_WrappedResidual.nm_coef" class="md-nav__link">
    nm_coef
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_GNP_Factory" class="md-nav__link">
    DP_GNP_Factory()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.clip_gradient" class="md-nav__link">
    clip_gradient()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.make_residuals" class="md-nav__link">
    make_residuals()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../losses/" class="md-nav__link">
        lipdp.losses module
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../model/" class="md-nav__link">
        lipdp.model module
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/" class="md-nav__link">
        lipdp.pipeline module
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../sensitivity/" class="md-nav__link">
        lipdp.sensitivity module
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers" class="md-nav__link">
    lipdp.layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.AddBias" class="md-nav__link">
    AddBias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer" class="md-nav__link">
    DPLayer
  </a>
  
    <nav class="md-nav" aria-label="DPLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.backpropagate_inputs" class="md-nav__link">
    backpropagate_inputs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.backpropagate_params" class="md-nav__link">
    backpropagate_params()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DPLayer.propagate_inputs" class="md-nav__link">
    propagate_inputs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_AddBias" class="md-nav__link">
    DP_AddBias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_BoundedInput" class="md-nav__link">
    DP_BoundedInput
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_ClipGradient" class="md-nav__link">
    DP_ClipGradient
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_MaxPool2D" class="md-nav__link">
    DP_MaxPool2D
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_ScaledL2NormPooling2D" class="md-nav__link">
    DP_ScaledL2NormPooling2D
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_WrappedResidual" class="md-nav__link">
    DP_WrappedResidual
  </a>
  
    <nav class="md-nav" aria-label="DP_WrappedResidual">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lipdp.layers.DP_WrappedResidual.nm_coef" class="md-nav__link">
    nm_coef
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.DP_GNP_Factory" class="md-nav__link">
    DP_GNP_Factory()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.clip_gradient" class="md-nav__link">
    clip_gradient()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lipdp.layers.make_residuals" class="md-nav__link">
    make_residuals()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>lipdp.layers module</h1>

<div class="doc doc-object doc-module">


<a id="lipdp.layers"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.AddBias" class="doc doc-heading">
        <code>AddBias</code>


<a href="#lipdp.layers.AddBias" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="tensorflow">tf</span>.<span title="tensorflow.keras">keras</span>.<span title="tensorflow.keras.layers">layers</span>.<span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p>Adds a bias to the input.</p>
<p>Remark: the euclidean norm of the bias must be bounded in advance.
Note that this is the euclidean norm of the whole bias vector, not
the norm of each element of the bias vector.</p>
<p>Warning: beware zero gradients outside the ball of norm norm_max.
In the future, we might choose a smoother projection on the ball to ensure
that the gradient remains non zero outside the ball.</p>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AddBias</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a bias to the input.</span>

<span class="sd">    Remark: the euclidean norm of the bias must be bounded in advance.</span>
<span class="sd">    Note that this is the euclidean norm of the whole bias vector, not</span>
<span class="sd">    the norm of each element of the bias vector.</span>

<span class="sd">    Warning: beware zero gradients outside the ball of norm norm_max.</span>
<span class="sd">    In the future, we might choose a smoother projection on the ball to ensure</span>
<span class="sd">    that the gradient remains non zero outside the ball.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_max</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_max</span> <span class="o">=</span> <span class="n">norm_max</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># parametrize the bias so it belongs to a ball of norm norm_max.</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_max</span><span class="p">)</span>  <span class="c1"># 1-Lipschitz operation.</span>
        <span class="k">return</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">bias</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DPLayer" class="doc doc-heading">
        <code>DPLayer</code>


<a href="#lipdp.layers.DPLayer" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">

  
      <p>Wrapper for created differentially private layers, instanciates abstract methods
use for computing the bounds of the gradient relatively to the parameters and to the
input.</p>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DPLayer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper for created differentially private layers, instanciates abstract methods</span>
<span class="sd">    use for computing the bounds of the gradient relatively to the parameters and to the</span>
<span class="sd">    input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Corresponds to the Lipschitz constant of the output wrt the parameters,</span>
<span class="sd">            i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_bound: Maximum norm of input.</span>
<span class="sd">            gradient_bound: Maximum norm of gradients (co-tangent vector)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Maximum norm of tangent vector.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,</span>
<span class="sd">            i.e. multiply by the Lipschitz constant of the output wrt input.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_bound: Maximum norm of input.</span>
<span class="sd">            gradient_bound: Maximum norm of gradients (co-tangent vector)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Maximum norm of tangent vector.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Maximum norm of output of element.</span>

<span class="sd">        Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="lipdp.layers.DPLayer.backpropagate_inputs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">backpropagate_inputs</span><span class="p">(</span><span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#lipdp.layers.DPLayer.backpropagate_inputs" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,
    i.e. multiply by the Lipschitz constant of the output wrt input.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input_bound</code></td>
          <td>
          </td>
          <td><p>Maximum norm of input.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gradient_bound</code></td>
          <td>
          </td>
          <td><p>Maximum norm of gradients (co-tangent vector)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>Maximum norm of tangent vector.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,</span>
<span class="sd">        i.e. multiply by the Lipschitz constant of the output wrt input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_bound: Maximum norm of input.</span>
<span class="sd">        gradient_bound: Maximum norm of gradients (co-tangent vector)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Maximum norm of tangent vector.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="lipdp.layers.DPLayer.backpropagate_params" class="doc doc-heading">
<code class="highlight language-python"><span class="n">backpropagate_params</span><span class="p">(</span><span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#lipdp.layers.DPLayer.backpropagate_params" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Corresponds to the Lipschitz constant of the output wrt the parameters,
    i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input_bound</code></td>
          <td>
          </td>
          <td><p>Maximum norm of input.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gradient_bound</code></td>
          <td>
          </td>
          <td><p>Maximum norm of gradients (co-tangent vector)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>Maximum norm of tangent vector.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Corresponds to the Lipschitz constant of the output wrt the parameters,</span>
<span class="sd">        i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_bound: Maximum norm of input.</span>
<span class="sd">        gradient_bound: Maximum norm of gradients (co-tangent vector)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Maximum norm of tangent vector.&quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="lipdp.layers.DPLayer.propagate_inputs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">propagate_inputs</span><span class="p">(</span><span class="n">input_bound</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#lipdp.layers.DPLayer.propagate_inputs" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximum norm of output of element.</p>
<p>Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.</p>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum norm of output of element.</span>

<span class="sd">    Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_AddBias" class="doc doc-heading">
        <code>DP_AddBias</code>


<a href="#lipdp.layers.DP_AddBias" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="lipdp.layers.AddBias" href="#lipdp.layers.AddBias">AddBias</a></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>

  
      <p>Adds a bias to the input.</p>
<p>The bias is projected on the ball of norm <code>norm_max</code> during training.
The projection on the ball is a 1-Lipschitz function, since the ball
is convex.</p>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_AddBias</span><span class="p">(</span><span class="n">AddBias</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a bias to the input.</span>

<span class="sd">    The bias is projected on the ball of norm `norm_max` during training.</span>
<span class="sd">    The projection on the ball is a 1-Lipschitz function, since the ball</span>
<span class="sd">    is convex.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">nm_coef</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nm_coef</span> <span class="o">=</span> <span class="n">nm_coef</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">gradient_bound</span>  <span class="c1"># clipping is a 1-Lipschitz operation.</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">gradient_bound</span>  <span class="c1"># adding is a 1-Lipschitz operation.</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_bound</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_max</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_BoundedInput" class="doc doc-heading">
        <code>DP_BoundedInput</code>


<a href="#lipdp.layers.DP_BoundedInput" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="tensorflow">tf</span>.<span title="tensorflow.keras">keras</span>.<span title="tensorflow.keras.layers">layers</span>.<span title="tensorflow.keras.layers.Layer">Layer</span></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>

  
      <p>Input layer that clips the input to a given norm.</p>
<p>Remark: every pipeline should start with this layer.</p>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>upper_bound</code></td>
          <td>
          </td>
          <td><p>Maximum norm of the input.</p></td>
        </tr>
        <tr>
          <td><code>enforce_clipping</code></td>
          <td>
          </td>
          <td><p>If True (default), the input is clipped to the given norm.</p></td>
        </tr>
    </tbody>
  </table>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_BoundedInput</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Input layer that clips the input to a given norm.</span>

<span class="sd">    Remark: every pipeline should start with this layer.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        upper_bound: Maximum norm of the input.</span>
<span class="sd">        enforce_clipping: If True (default), the input is clipped to the given norm.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">,</span> <span class="n">enforce_clipping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upper_bound</span> <span class="o">=</span> <span class="n">upper_bound</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enforce_clipping</span> <span class="o">=</span> <span class="n">enforce_clipping</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_clipping</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper_bound</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;InputLayer doesn&#39;t have parameters&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">gradient_bound</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">input_bound</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper_bound</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">upper_bound</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_ClipGradient" class="doc doc-heading">
        <code>DP_ClipGradient</code>


<a href="#lipdp.layers.DP_ClipGradient" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="tensorflow">tf</span>.<span title="tensorflow.keras">keras</span>.<span title="tensorflow.keras.layers">layers</span>.<span title="tensorflow.keras.layers.Layer">Layer</span></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>

  
      <p>Clips the gradient during the backward pass.</p>
<p>Behave like identity function during the forward pass.
The clipping is done automatically during the backward pass.</p>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>clip_value</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The maximum norm of the gradient.</p></td>
        </tr>
    </tbody>
  </table>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_ClipGradient</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Clips the gradient during the backward pass.</span>

<span class="sd">    Behave like identity function during the forward pass.</span>
<span class="sd">    The clipping is done automatically during the backward pass.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        clip_value (float): The maximum norm of the gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span> <span class="o">=</span> <span class="n">clip_value</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># the clipping is done elementwise</span>
        <span class="c1"># since REDUCTION=SUM_OVER_BATCH_SIZE, we need to divide by batch_size</span>
        <span class="c1"># to get the correct norm.</span>
        <span class="c1"># this makes the clipping independent of the batch size.</span>
        <span class="n">elementwise_clip_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">clip_gradient</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">elementwise_clip_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ClipGradient doesn&#39;t have parameters&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">gradient_bound</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_bound</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_MaxPool2D" class="doc doc-heading">
        <code>DP_MaxPool2D</code>


<a href="#lipdp.layers.DP_MaxPool2D" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="tensorflow">tf</span>.<span title="tensorflow.keras">keras</span>.<span title="tensorflow.keras.layers">layers</span>.<span title="tensorflow.keras.layers.MaxPool2D">MaxPool2D</span></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>

  
      <p>Max pooling layer that preserves the gradient norm.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>layer_cls</code></td>
          <td>
          </td>
          <td><p>Class of the layer to wrap.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>A differentially private layer that doesn't have parameters.</p></td>
        </tr>
    </tbody>
  </table>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_MaxPool2D</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Max pooling layer that preserves the gradient norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer_cls: Class of the layer to wrap.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A differentially private layer that doesn&#39;t have parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span>
        <span class="p">),</span> <span class="s2">&quot;Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.&quot;</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Layer doesn&#39;t have parameters&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">gradient_bound</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_bound</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_ScaledL2NormPooling2D" class="doc doc-heading">
        <code>DP_ScaledL2NormPooling2D</code>


<a href="#lipdp.layers.DP_ScaledL2NormPooling2D" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="deel.lip">lip</span>.<span title="deel.lip.layers">layers</span>.<span title="deel.lip.layers.ScaledL2NormPooling2D">ScaledL2NormPooling2D</span></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>

  
      <p>Max pooling layer that preserves the gradient norm.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>layer_cls</code></td>
          <td>
          </td>
          <td><p>Class of the layer to wrap.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>A differentially private layer that doesn't have parameters.</p></td>
        </tr>
    </tbody>
  </table>


        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_ScaledL2NormPooling2D</span><span class="p">(</span><span class="n">lip</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ScaledL2NormPooling2D</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Max pooling layer that preserves the gradient norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer_cls: Class of the layer to wrap.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A differentially private layer that doesn&#39;t have parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span>
        <span class="p">),</span> <span class="s2">&quot;Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.&quot;</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Layer doesn&#39;t have parameters&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">gradient_bound</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_bound</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="lipdp.layers.DP_WrappedResidual" class="doc doc-heading">
        <code>DP_WrappedResidual</code>


<a href="#lipdp.layers.DP_WrappedResidual" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="tensorflow">tf</span>.<span title="tensorflow.keras">keras</span>.<span title="tensorflow.keras.layers">layers</span>.<span title="tensorflow.keras.layers.Layer">Layer</span></code>, <code><a class="autorefs autorefs-internal" title="lipdp.layers.DPLayer" href="#lipdp.layers.DPLayer">DPLayer</a></code></p>



        <details class="quote">
          <summary>Source code in <code>lipdp/layers.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DP_WrappedResidual</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">block</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">i2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">i2</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span>

    <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_bound</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradient_bound</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">input_bound</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">gradient_bound</span>
        <span class="n">g2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">backpropagate_params</span><span class="p">(</span><span class="n">i2</span><span class="p">,</span> <span class="n">g2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">g2</span>

    <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_bound</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradient_bound</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">input_bound</span>
        <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">gradient_bound</span>
        <span class="n">g2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">backpropagate_inputs</span><span class="p">(</span><span class="n">i2</span><span class="p">,</span> <span class="n">g2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span>

    <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_bound</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">input_bound</span>
        <span class="n">i2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">propagate_inputs</span><span class="p">(</span><span class="n">i2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span>

    <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">has_parameters</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nm_coef</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the norm multiplier coefficient of the layer.</span>

<span class="sd">        Remark: this is a property to mimic the behavior of an attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">nm_coef</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="lipdp.layers.DP_WrappedResidual.nm_coef" class="doc doc-heading">
<code class="highlight language-python"><span class="n">nm_coef</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lipdp.layers.DP_WrappedResidual.nm_coef" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Returns the norm multiplier coefficient of the layer.</p>
<p>Remark: this is a property to mimic the behavior of an attribute.</p>
  </div>

</div>





  </div>

  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="lipdp.layers.DP_GNP_Factory" class="doc doc-heading">
<code class="highlight language-python"><span class="n">DP_GNP_Factory</span><span class="p">(</span><span class="n">layer_cls</span><span class="p">)</span></code>

<a href="#lipdp.layers.DP_GNP_Factory" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
  
      <p>Factory for creating differentially private gradient norm preserving layers that don't have parameters.</p>
<p>Remark: the layer is assumed to be GNP.
This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1).
Pllease ensure that the layer is GNP before using this factory.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>layer_cls</code></td>
          <td>
          </td>
          <td><p>Class of the layer to wrap.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>A differentially private layer that doesn't have parameters.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">DP_GNP_Factory</span><span class="p">(</span><span class="n">layer_cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Factory for creating differentially private gradient norm preserving layers that don&#39;t have parameters.</span>

<span class="sd">    Remark: the layer is assumed to be GNP.</span>
<span class="sd">    This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1).</span>
<span class="sd">    Pllease ensure that the layer is GNP before using this factory.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer_cls: Class of the layer to wrap.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A differentially private layer that doesn&#39;t have parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">DP_GNP</span><span class="p">(</span><span class="n">layer_cls</span><span class="p">,</span> <span class="n">DPLayer</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">backpropagate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Layer doesn&#39;t have parameters&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">backpropagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">,</span> <span class="n">gradient_bound</span><span class="p">):</span>
            <span class="k">return</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">gradient_bound</span>

        <span class="k">def</span> <span class="nf">propagate_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bound</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">input_bound</span>

        <span class="k">def</span> <span class="nf">has_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="n">DP_GNP</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;DP_</span><span class="si">{</span><span class="n">layer_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">DP_GNP</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="lipdp.layers.clip_gradient" class="doc doc-heading">
<code class="highlight language-python"><span class="n">clip_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">)</span></code>

<a href="#lipdp.layers.clip_gradient" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
  
      <p>Clips the gradient during the backward pass.</p>
<p>Behave like identity function during the forward pass.</p>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@tf</span><span class="o">.</span><span class="n">custom_gradient</span>
<span class="k">def</span> <span class="nf">clip_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Clips the gradient during the backward pass.</span>

<span class="sd">    Behave like identity function during the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">dy</span><span class="p">):</span>
        <span class="c1"># clip by norm each row</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dy</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
        <span class="n">clipped_dy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">clipped_dy</span><span class="p">,</span> <span class="kc">None</span>  <span class="c1"># No gradient for clip_value</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_fn</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="lipdp.layers.make_residuals" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_residuals</span><span class="p">(</span><span class="n">merge_policy</span><span class="p">,</span> <span class="n">wrapped_layers</span><span class="p">)</span></code>

<a href="#lipdp.layers.make_residuals" class="headerlink" title="Permanent link">&para;</a></h2>


  <div class="doc doc-contents ">
  
      <p>Returns a list of layers that implement a residual block.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>merge_policy</code></td>
          <td>
          </td>
          <td><p>either "add" or "1-lip-add".</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>wrapped_layers</code></td>
          <td>
          </td>
          <td><p>a list of layers that will be wrapped in residual blocks.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>A list of layers that implement a residual block.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>lipdp/layers.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_residuals</span><span class="p">(</span><span class="n">merge_policy</span><span class="p">,</span> <span class="n">wrapped_layers</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a list of layers that implement a residual block.</span>

<span class="sd">    Args:</span>
<span class="sd">        merge_policy: either &quot;add&quot; or &quot;1-lip-add&quot;.</span>
<span class="sd">        wrapped_layers: a list of layers that will be wrapped in residual blocks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of layers that implement a residual block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DP_SplitResidual</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">wrapped_layers</span><span class="p">:</span>
        <span class="n">residual_block</span> <span class="o">=</span> <span class="n">DP_WrappedResidual</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">residual_block</span><span class="p">)</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DP_MergeResidual</span><span class="p">(</span><span class="n">merge_policy</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">layers</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../js/custom.js"></script>
      
    
  </body>
</html>