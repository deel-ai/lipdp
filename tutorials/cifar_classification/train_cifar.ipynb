{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA 11.2 at /usr/local/cuda-11.2.\n",
      "Using CUDNN 8.1.0 at /usr/local/cudnn/11.2-v8.1.0\n"
     ]
    }
   ],
   "source": [
    "! set_cuda_version 11.2 8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "\n",
    "import deel.lipdp.layers as DP_layers\n",
    "import deel.lipdp.losses as DP_losses\n",
    "from deel.lipdp.pipeline import bound_clip_value\n",
    "from deel.lipdp.pipeline import load_and_prepare_data\n",
    "from deel.lipdp.sensitivity import get_max_epochs\n",
    "from deel.lipdp.model import DP_Accountant\n",
    "from deel.lipdp.model import DP_Sequential\n",
    "from deel.lipdp.model import DPParameters\n",
    "from deel.lipdp.model import AdaptiveLossGradientClipping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data :\n",
    "\n",
    "It is important to import the data with the right DP parameters to account properly for the privacy guarantees of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations = [\n",
    "#     keras_cv.layers.RandomRotation(0.2, fill_mode=\"reflect\", interpolation=\"bilinear\"),\n",
    "#     keras_cv.layers.RandomTranslation(\n",
    "#         0.2, 0.2, fill_mode=\"reflect\", interpolation=\"bilinear\"\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "ds_train, ds_test, dataset_metadata = load_and_prepare_data(\n",
    "    \"cifar10\",\n",
    "    batch_size=750,\n",
    "    colorspace=\"HSV\",\n",
    "    # augmentations=augmentations,\n",
    "    drop_remainder=True,  # accounting assumes fixed batch size\n",
    "    bound_fct=bound_clip_value(\n",
    "        10.0\n",
    "    ),  # clipping preprocessing allows to control input bound\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pay attention to the fact that the effective batch size in memory will be batch_size $\\times$ len(augmentations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the DP parameters :\n",
    "\n",
    "We also need to declare explicitly the parameters of the DP training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_parameters = DPParameters(\n",
    "    noisify_strategy=\"global\",\n",
    "    noise_multiplier=0.75,\n",
    "    delta=1e-5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model :\n",
    "\n",
    "We use a simple convolutive network to classify on the MNIST dataset. We add a loss gradient clipping layer at the end of our network for more tightness on our gradient's upper bound. Therefore allowing for better results with one less hyperparameter to tune for dynamically chosen clipping constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas.massena/Code/DEBUG/dp-lipschitz/lipdp_dev_env/lib/python3.9/site-packages/deel/lip/model.py:74: UserWarning: Sequential model contains a layer which is not a 1-Lipschitz layer: dp__bounded_input_2\n",
      "  warn(_msg_not_lip.format(layer.name))\n",
      "/home/thomas.massena/Code/DEBUG/dp-lipschitz/lipdp_dev_env/lib/python3.9/site-packages/deel/lip/model.py:74: UserWarning: Sequential model contains a layer which is not a 1-Lipschitz layer: dp__clip_gradient_2\n",
      "  warn(_msg_not_lip.format(layer.name))\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    DP_layers.DP_BoundedInput(\n",
    "        input_shape=dataset_metadata.input_shape,\n",
    "        upper_bound=dataset_metadata.max_norm,\n",
    "    ),\n",
    "    DP_layers.DP_SpectralConv2D(\n",
    "        filters=32, kernel_size=3, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_Flatten(),\n",
    "    DP_layers.DP_SpectralDense(\n",
    "        units=512, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_GroupSort(2),\n",
    "    DP_layers.DP_SpectralDense(\n",
    "        units=10, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "    ),\n",
    "    DP_layers.DP_ClipGradient(\n",
    "        epsilon=1, mode=\"dynamic_svt\", patience=5\n",
    "    )\n",
    "]\n",
    "\n",
    "model = DP_Sequential(\n",
    "    layers=layers, dp_parameters=dp_parameters, dataset_metadata=dataset_metadata\n",
    ")\n",
    "\n",
    "loss = DP_losses.DP_TauCategoricalCrossentropy(14.5)\n",
    "\n",
    "# Compatible with any kind of non-private optimizer : \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=opt,\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the desired DP guarantees :\n",
    "\n",
    "We compute the budget of epochs needed to yields the DP guarantees that you desire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch bounds = (0, 512.0) and epsilon = 121.24060720498198 at epoch 512.0\n",
      "epoch bounds = (0, 256.0) and epsilon = 66.38218380431994 at epoch 256.0\n",
      "epoch bounds = (0, 128.0) and epsilon = 37.92323089410726 at epoch 128.0\n",
      "epoch bounds = (0, 64.0) and epsilon = 23.425990726099066 at epoch 64.0\n",
      "epoch bounds = (0, 32.0) and epsilon = 14.191037676444132 at epoch 32.0\n",
      "epoch bounds = (0, 16.0) and epsilon = 9.561652236093542 at epoch 16.0\n",
      "epoch bounds = (8.0, 16.0) and epsilon = 6.218921347499752 at epoch 8.0\n",
      "epoch bounds = (8.0, 12.0) and epsilon = 8.108216363628433 at epoch 12.0\n",
      "epoch bounds = (10.0, 12.0) and epsilon = 7.6115104852854385 at epoch 10.0\n",
      "epoch bounds = (11.0, 12.0) and epsilon = 7.859863424456936 at epoch 11.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = get_max_epochs(8.0, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model : \n",
    "\n",
    "The training process is called through the model.fit attribute. We use the following callbacks : \n",
    "\n",
    "- **DP_Accountant** (log_fn) : accounts for the privacy guarantees after each epoch of training (*log_fn* makes it compatible with W&B logging).\n",
    "- **DP_AdaptiveGradientClipping** (ds_train, patience) : automatically updates the losses's gradient clipping constant every *patience* steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On train begin : \n",
      "Initial value is now equal to lipschitz constant of loss:  tf.Tensor(1.4142135, shape=(), dtype=float32)\n",
      "Epoch 1/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.2234\n",
      " (3.334448232597648, 1e-05)-DP guarantees for epoch 1 \n",
      "\n",
      "updated_clip_value :  1.006512355202473\n",
      "66/66 [==============================] - 15s 198ms/step - loss: 0.1467 - accuracy: 0.2234 - val_loss: 0.1382 - val_accuracy: 0.2878\n",
      "Epoch 2/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.3024\n",
      " (3.5845675174158074, 1e-05)-DP guarantees for epoch 2 \n",
      "\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.1357 - accuracy: 0.3024 - val_loss: 0.1330 - val_accuracy: 0.3275\n",
      "Epoch 3/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.3275\n",
      " (3.832920482489297, 1e-05)-DP guarantees for epoch 3 \n",
      "\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.1321 - accuracy: 0.3275 - val_loss: 0.1310 - val_accuracy: 0.3349\n",
      "Epoch 4/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.3352\n",
      " (4.081273443586172, 1e-05)-DP guarantees for epoch 4 \n",
      "\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.1303 - accuracy: 0.3352 - val_loss: 0.1292 - val_accuracy: 0.3448\n",
      "Epoch 5/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.3439\n",
      " (5.225509601937803, 1e-05)-DP guarantees for epoch 5 \n",
      "\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.1291 - accuracy: 0.3439 - val_loss: 0.1282 - val_accuracy: 0.3487\n",
      "Epoch 6/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.3519\n",
      " (5.47386252998526, 1e-05)-DP guarantees for epoch 6 \n",
      "\n",
      "updated_clip_value :  1.0305780514590723\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.1281 - accuracy: 0.3519 - val_loss: 0.1275 - val_accuracy: 0.3583\n",
      "Epoch 7/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.3552\n",
      " (5.722215469156756, 1e-05)-DP guarantees for epoch 7 \n",
      "\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.1273 - accuracy: 0.3552 - val_loss: 0.1270 - val_accuracy: 0.3570\n",
      "Epoch 8/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.3559\n",
      " (5.970568408328255, 1e-05)-DP guarantees for epoch 8 \n",
      "\n",
      "66/66 [==============================] - 12s 178ms/step - loss: 0.1268 - accuracy: 0.3559 - val_loss: 0.1266 - val_accuracy: 0.3595\n",
      "Epoch 9/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.3592\n",
      " (6.218921347499752, 1e-05)-DP guarantees for epoch 9 \n",
      "\n",
      "66/66 [==============================] - 12s 178ms/step - loss: 0.1263 - accuracy: 0.3592 - val_loss: 0.1262 - val_accuracy: 0.3585\n",
      "Epoch 10/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.3614\n",
      " (7.363157546113941, 1e-05)-DP guarantees for epoch 10 \n",
      "\n",
      "66/66 [==============================] - 12s 178ms/step - loss: 0.1260 - accuracy: 0.3614 - val_loss: 0.1260 - val_accuracy: 0.3647\n",
      "Epoch 11/11\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.3653\n",
      " (7.6115104852854385, 1e-05)-DP guarantees for epoch 11 \n",
      "\n",
      "updated_clip_value :  1.027746793075943\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.1257 - accuracy: 0.3653 - val_loss: 0.1257 - val_accuracy: 0.3649\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    DP_Accountant(log_fn=\"logging\"),\n",
    "    AdaptiveLossGradientClipping(\n",
    "        ds_train=ds_train\n",
    "    ),  # DO NOT USE THIS CALLBACK WHEN mode != \"dynamic_svt\"\n",
    "]\n",
    "\n",
    "hist = model.fit(\n",
    "    ds_train,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipdp_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
